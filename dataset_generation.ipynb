{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b158d2-3926-4d4a-93c1-b19658bc3d9d",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b2453c-7d62-46e2-8bf9-ee5de0343368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slavik/Python/tasks/task1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa07e5-4186-4465-abaa-e9a990f97666",
   "metadata": {},
   "source": [
    "# Defining prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f29bd1-5b7a-4196-9bc9-c0a5fee0a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = SystemMessage(content=\"\"\"\n",
    "You are a dataset generator for named entity recognition (NER). Your task is to generate 5 unique, natural-sounding English sentences.\n",
    "Return your output **ONLY** as a JSON array.\n",
    "\n",
    "Requirements:\n",
    "1. Each sentence may include one or more mountain names from the provided list.\n",
    "2. Every mountain in the input list must appear **at least once** across the 5 sentences.\n",
    "3. Wrap every mountain name exactly with <mon> and </mon> tags. Example:\n",
    "   \"The climbers scaled <mon>Mount Everest</mon> and rested near <mon>K2</mon>.\"\n",
    "4. Do NOT include start or end character indices—only inline tags.\n",
    "5. Use diverse forms of mountain names: \"Mount X\", \"Mt. X\", \"X\", \"Monte X\", or local variants.\n",
    "6. Include sentences with punctuation, parentheses, lists (\"... and ...\"), abbreviations (\"Mt.\"), and possessive forms (\"K2's prominence\").\n",
    "7. Include some negative examples: sentences may contain the word \"mountain\" metaphorically or place names that are not mountains, but do NOT wrap them in <mon> tags.\n",
    "8. Include a few sentences with **no mountain names at all**.\n",
    "\n",
    "Example:\n",
    "[\n",
    "  \"The climbers scaled <mon>Mount Everest</mon> and rested near <mon>K2</mon>.\",\n",
    "  \"After conquering <mon>Kangchenjunga</mon>, they turned their attention to <mon>Lhotse</mon>.\"\n",
    "]\n",
    "\"\"\")\n",
    "\n",
    "HUMAN_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    input_variables=[\"mountains\"],\n",
    "    template=\"\"\"\n",
    "Generate 5 unique sentences using the following mountains: {mountains}.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ce704-9493-4a29-8b16-e69cfea99d92",
   "metadata": {},
   "source": [
    "# Setting up LLM. Local model in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72202564-b6c8-4386-8618-2f9df2c33263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:8000/v1\",\n",
    "    api_key=\"none\",\n",
    "    model=\"local\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d819975-5ccf-42d6-8e8b-514e2982a88d",
   "metadata": {},
   "source": [
    "# Fetching mountain names from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4dcf68-50f0-49c7-81c0-55c382a0d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_mountains_by_elevation\"\n",
    "HEADERS = {\n",
    "    \"User-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def fetch_mountains():\n",
    "    mountains = []\n",
    "    r = requests.get(URL, headers=HEADERS)\n",
    "    html = BeautifulSoup(r.content, \"lxml\")\n",
    "    tables = html.find_all(\"tbody\")\n",
    "    for table in tables:\n",
    "        elems = table.find_all(\"tr\")\n",
    "        for elem in elems[1:]:\n",
    "            elem = elem.find_all(\"td\")\n",
    "            link = elem[0].find(\"a\")\n",
    "            if link is None:\n",
    "                name = elem[0].text.strip()\n",
    "            else:\n",
    "                name = link.get(\"title\")\n",
    "            mountains.append(name)\n",
    "\n",
    "    return mountains\n",
    "\n",
    "def fetch_mountains_by_region():\n",
    "    mountains_by_region = defaultdict(list)\n",
    "    r = requests.get(URL, headers=HEADERS)\n",
    "    html = BeautifulSoup(r.content, \"lxml\")\n",
    "    tables = html.find_all(\"tbody\")\n",
    "    for table in tables:\n",
    "        elems = table.find_all(\"tr\")\n",
    "        for elem in elems[1:]:\n",
    "            elem = elem.find_all(\"td\")\n",
    "            link = elem[0].find(\"a\")\n",
    "            if link is None:\n",
    "                name = elem[0].text.strip()\n",
    "            else:\n",
    "                name = link.get(\"title\")\n",
    "            region = elem[-1].text.strip()\n",
    "            mountains_by_region[region].append(name)\n",
    "\n",
    "    return mountains_by_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b1195-7c9c-4dce-9568-c9ea09eaca00",
   "metadata": {},
   "source": [
    "# Defining parse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5684a427-4daa-482d-9472-0a7bb2ef3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(output: str):\n",
    "    \"\"\"\n",
    "    Parse mountain tags from LLM response into a Dict\n",
    "\n",
    "    Args:\n",
    "        output (str): Raw response from LLM\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed output\n",
    "    \"\"\"\n",
    "    # Parsing Json \n",
    "    #pattern = r\"```(?:[a-zA-Z0-9_+-]+)?\\n?(.*?)```\"\n",
    "    #match = re.search(pattern, output, flags=re.DOTALL)\n",
    "    #content = json.loads(match.group(1))\n",
    "    \n",
    "    content = json.loads(output)\n",
    "    result = []\n",
    "    for text in content:\n",
    "        entities = []\n",
    "        clean_text = \"\"\n",
    "        last_index = 0\n",
    "\n",
    "        # Parsing <mon></mon> tags and removing them\n",
    "        pattern = r\"<mon>(.*?)</mon>\"\n",
    "        for mon_match in re.finditer(pattern, text):\n",
    "            start, end = mon_match.span()\n",
    "            name = mon_match.group(1)\n",
    "\n",
    "            # Adding text before tag\n",
    "            clean_text += text[last_index:start]\n",
    "            entity_start = len(clean_text)\n",
    "            # Adding name\n",
    "            clean_text += name\n",
    "            entity_end = len(clean_text)\n",
    "            \n",
    "            entities.append({\n",
    "                \"start\": entity_start,\n",
    "                \"end\": entity_end,\n",
    "                \"label\": \"MOUNTAIN\"\n",
    "            })\n",
    "\n",
    "            last_index = end\n",
    "\n",
    "        # Adding the rest of the text\n",
    "        clean_text += text[last_index:]\n",
    "            \n",
    "        result.append({\n",
    "            \"text\": clean_text,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f14e0c-9546-4a94-8a0b-3e29c4c67083",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "\n",
    "- Takes random samples from mountains\n",
    "- Gives to LLM\n",
    "- Tries to parse it\n",
    "- Move to dataset\n",
    "- Dumps results to disk that we do not loose them if something breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7cf0679-c2d9-4cfd-b574-509ed2b1f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountains = fetch_mountains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d8e9dd-8748-4c5d-896f-5a647fb61f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 19-54-01.json:  19%|███████▌                                | 312/1651 [07:25<32:12,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-00-09.json:  34%|█████████████▊                          | 568/1651 [13:33<24:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-01-02.json:  36%|██████████████▎                         | 592/1651 [14:13<28:10,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-01-28.json:  37%|██████████████▉                         | 616/1651 [14:54<26:46,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-03-27.json:  42%|████████████████▊                       | 696/1651 [16:49<22:05,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-05-25.json:  47%|██████████████████▋                     | 772/1651 [18:43<20:41,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-07-00.json:  51%|████████████████████▍                   | 844/1651 [20:30<20:25,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-07-50.json:  53%|█████████████████████                   | 868/1651 [21:10<19:44,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-11-40.json:  62%|████████████████████████▊               | 1024/1651 [24:53<15:25,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-11-40.json:  62%|████████████████████████▊               | 1024/1651 [24:59<15:25,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-15-07.json:  71%|████████████████████████████▍           | 1176/1651 [28:31<11:09,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-16-19.json:  74%|█████████████████████████████▌          | 1220/1651 [29:37<10:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-17-33.json:  77%|██████████████████████████████▌         | 1264/1651 [30:44<09:09,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-17-33.json:  77%|██████████████████████████████▋         | 1268/1651 [30:55<11:03,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-18-27.json:  78%|███████████████████████████████▍        | 1296/1651 [31:39<08:25,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-24-10.json:  94%|█████████████████████████████████████▍  | 1544/1651 [37:33<02:37,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED TO PARSE: 1544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DUMP: dataset/ner_dataset_2025-11-16 20-27-00.json: 100%|████████████████████████████████████████| 1651/1651 [40:06<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "DUMP_PERIOD = 4 # LLM outputs in one dump\n",
    "MOUNTAINS_BATCH_SIZE = 4 # batch size\n",
    "DATASET_DIR = Path(\"dataset\") # Dataset directory\n",
    "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TOTAL_BATCHES = (len(mountains) + MOUNTAINS_BATCH_SIZE - 1) // MOUNTAINS_BATCH_SIZE\n",
    "\n",
    "i = 0\n",
    "\n",
    "dataset = []\n",
    "with tqdm(total=len(mountains), initial=i, bar_format='{l_bar}{bar:40}{r_bar}') as pbar:\n",
    "    while i < len(mountains):\n",
    "        try:\n",
    "            #start_idx = i * MOUNTAINS_BATCH_SIZE\n",
    "            #end_idx = (i + 1) * MOUNTAINS_BATCH_SIZE\n",
    "            sampled_mountains = mountains[i:i + MOUNTAINS_BATCH_SIZE]\n",
    "    \n",
    "            human_prompt = HUMAN_PROMPT_TEMPLATE.format(mountains=sampled_mountains)\n",
    "            response = llm.invoke([\n",
    "                SYSTEM_PROMPT,\n",
    "                human_prompt\n",
    "            ])\n",
    "            \n",
    "            try:\n",
    "                dataset.extend(parse(response.content))\n",
    "            except json.JSONDecodeError:\n",
    "                tqdm.write(f\"FAILED TO PARSE: {i}\")\n",
    "                continue\n",
    "        \n",
    "            if (len(dataset) >= MOUNTAINS_BATCH_SIZE * DUMP_PERIOD or i + len(sampled_mountains) >= len(mountains)):\n",
    "                file_name = DATASET_DIR / f\"ner_dataset_{datetime.now().strftime('%Y-%m-%d %H-%M-%S')}.json\"\n",
    "                with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(dataset, f, indent=2)\n",
    "                pbar.set_description(f\"DUMP: {file_name}\")\n",
    "                dataset = []\n",
    "            \n",
    "            pbar.update(len(sampled_mountains))\n",
    "            i += len(sampled_mountains)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopping, saving current dataset\")\n",
    "            if dataset:\n",
    "                file_name = DATASET_DIR / f\"ner_dataset_{datetime.now().strftime('%Y-%m-%d %H-%M-%S')}.json\"\n",
    "                with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(dataset, f, indent=2)\n",
    "                print(f\"Saved: {file_name}\")\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f90b5a-664b-494e-a638-6b1fa0dcdc54",
   "metadata": {},
   "source": [
    "# Merge all dumps into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e473d5f0-3a51-4465-8410-23acc4b7af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"dataset.json\")\n",
    "\n",
    "result = []\n",
    "for item in Path(DATASET_DIR).rglob(\"ner_dataset_*.json\"):\n",
    "    with open(item, \"r\", encoding=\"utf-8\") as f:\n",
    "        result.extend(json.load(f))\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba28022-dfcf-411e-a7a8-97cfeb7bba1e",
   "metadata": {},
   "source": [
    "# Defining auxiliary functions for dataset conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbd8754-42be-4f07-8f79-dfaed24d3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "LABEL_B = \"B-MOUNTAIN\"\n",
    "LABEL_I = \"I-MOUNTAIN\"\n",
    "LABEL_O = \"O\"\n",
    "\n",
    "label2id = {LABEL_O: 0, LABEL_B: 1, LABEL_I: 2}\n",
    "\n",
    "\n",
    "def load_json(path: Path) -> List[Dict]:\n",
    "    text = path.read_text(encoding=\"utf-8\").strip()\n",
    "    if not text:\n",
    "        raise ValueError(f\"Input file {path} is empty.\")\n",
    "    # try JSON array\n",
    "    data = json.loads(text)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"JSON top-level is not a list.\")\n",
    "\n",
    "def convert_spans_to_token_labels(data: List[Dict], tokenizer, max_length: int):\n",
    "    \"\"\"\n",
    "    Returns dict with keys:\n",
    "      - input_ids, attention_mask, labels (list of ints same length as input_ids; -100 for ignored)\n",
    "      - tokens (for inspection)\n",
    "      - offsets (for inspection)\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "        \"tokens\": [],\n",
    "        \"text\": []\n",
    "    }\n",
    "    problems = []\n",
    "    for idx, item in enumerate(data):\n",
    "        text = item.get(\"text\", \"\")\n",
    "        ents = item.get(\"entities\", [])\n",
    "        # char labels: 0 = O, 1 = entity\n",
    "        char_labels = [0] * len(text)\n",
    "        for ent in ents:\n",
    "            s, e = ent.get(\"start\"), ent.get(\"end\")\n",
    "            if not (isinstance(s, int) and isinstance(e, int) and 0 <= s < e <= len(text)):\n",
    "                problems.append((idx, \"bad_entity_offsets\", ent))\n",
    "                continue\n",
    "            for i in range(s, e):\n",
    "                char_labels[i] = 1\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        offsets = enc.pop(\"offset_mapping\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
    "        label_ids = []\n",
    "        for (tok_start, tok_end), token in zip(offsets, tokens):\n",
    "            # special tokens: some fast tokenizers encode them with (0,0) offsets\n",
    "            if tok_start == tok_end == 0:\n",
    "                label_ids.append(-100)\n",
    "                continue\n",
    "            # ensure offsets inside text bounds\n",
    "            tok_start = min(tok_start, len(text))\n",
    "            tok_end = min(tok_end, len(text))\n",
    "            if tok_start >= tok_end:\n",
    "                label_ids.append(-100)\n",
    "                continue\n",
    "            # determine overlap with char_labels\n",
    "            inside = any(char_labels[tok_start:tok_end])\n",
    "            if not inside:\n",
    "                label_ids.append(label2id[LABEL_O])\n",
    "                continue\n",
    "            # is token the start of an entity? i.e., char at tok_start is entity and either at text start or previous char not entity\n",
    "            is_start = False\n",
    "            if tok_start < len(char_labels) and char_labels[tok_start] == 1:\n",
    "                if tok_start == 0 or char_labels[tok_start - 1] == 0:\n",
    "                    is_start = True\n",
    "            # fallback: if no char at tok_start is labeled but earlier chars inside token are entity (rare), treat as I\n",
    "            if is_start:\n",
    "                label_ids.append(label2id[LABEL_B])\n",
    "            else:\n",
    "                label_ids.append(label2id[LABEL_I])\n",
    "        out[\"input_ids\"].append(enc[\"input_ids\"])\n",
    "        out[\"attention_mask\"].append(enc[\"attention_mask\"])\n",
    "        out[\"labels\"].append(label_ids)\n",
    "        out[\"tokens\"].append(tokens)\n",
    "        out[\"text\"].append(text)\n",
    "    return out, problems\n",
    "\n",
    "def write_conll(out_prefix: str, tokens_list: List[List[str]], labels_list: List[List[int]], tokenizer):\n",
    "    path = Path(f\"{out_prefix}.conll\")\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for tokens, labs in zip(tokens_list, labels_list):\n",
    "            for tok, lab in zip(tokens, labs):\n",
    "                if lab == -100:\n",
    "                    continue\n",
    "                # convert lab id -> tag\n",
    "                if lab == label2id[LABEL_O]:\n",
    "                    tag = \"O\"\n",
    "                elif lab == label2id[LABEL_B]:\n",
    "                    tag = LABEL_B\n",
    "                elif lab == label2id[LABEL_I]:\n",
    "                    tag = LABEL_I\n",
    "                else:\n",
    "                    tag = \"O\"\n",
    "                f.write(f\"{tok}\\t{tag}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    return str(path)\n",
    "\n",
    "def save_hf_dataset(out_prefix: str, tokenized: Dict):\n",
    "    try:\n",
    "        from datasets import Dataset\n",
    "    except Exception:\n",
    "        print(\"`datasets` not installed; skipping saving Hugging Face dataset. Install with `pip install datasets` to enable.\")\n",
    "        return None\n",
    "    ds = Dataset.from_dict({\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"labels\"],\n",
    "        \"text\": tokenized[\"text\"]\n",
    "    })\n",
    "    # Save locally\n",
    "    outdir = f\"{out_prefix}.hf_dataset\"\n",
    "    ds.save_to_disk(outdir)\n",
    "    return outdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c6d65-13cf-431a-b26c-f9c234054121",
   "metadata": {},
   "source": [
    "# Converting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70496fff-da0e-42f7-ac30-1d6b0d6afca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2070 examples from dataset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slavik/Python/tasks/task1/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2070 examples to token labels. Problematic entities: 0\n",
      "Wrote inspection CoNLL to: mountains_prepared.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2070/2070 [00:00<00:00, 382055.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face dataset saved to: mountains_prepared.hf_dataset\n",
      "\n",
      "=== Example token/label outputs (first 3) ===\n",
      "\n",
      "TEXT: The group planned to hike up Serles, then traverse Watzmann the following week.\n",
      "The:O group:O planned:O to:O hike:O up:O Ser:B-MOUNTAIN ##les:I-MOUNTAIN ,:O then:O t:O ##raverse:O W:B-MOUNTAIN ##at:I-MOUNTAIN ##zman:I-MOUNTAIN ##n:I-MOUNTAIN the:O following:O week:O .:O\n",
      "\n",
      "TEXT: Although she's climbed Boston Peak, she's still afraid of heights, unlike her friend who conquered Schiahorn last summer.\n",
      "Although:O she:O ':O s:O climbed:O Boston:B-MOUNTAIN Peak:I-MOUNTAIN ,:O she:O ':O s:O still:O afraid:O of:O heights:O ,:O unlike:O her:O friend:O who:O conquered:O Sc:B-MOUNTAIN ##hia:I-MOUNTAIN ##horn:I-MOUNTAIN last:O summer:O .:O\n",
      "\n",
      "TEXT: The peak of Watzmann, also known as the 'Dentist's Chair', was her final destination, having already summited Boston Peak.\n",
      "The:O peak:O of:O W:B-MOUNTAIN ##at:I-MOUNTAIN ##zman:I-MOUNTAIN ##n:I-MOUNTAIN ,:O also:O known:O as:O the:O ':O Den:O ##tist:O ':O s:O Chair:O ':O ,:O was:O her:O final:O destination:O ,:O having:O already:O summit:O ##ed:O Boston:B-MOUNTAIN Peak:I-MOUNTAIN .:O\n",
      "\n",
      "Label mapping for Trainer:\n",
      "label2id: {'O': 0, 'B-MOUNTAIN': 1, 'I-MOUNTAIN': 2}\n",
      "id2label: {'0': 'O', '1': 'B-MOUNTAIN', '2': 'I-MOUNTAIN'}\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "INPUT_JSON_PATH = \"dataset.json\"\n",
    "OUTPUT_PATH = \"mountains_prepared\"\n",
    "MODEL = \"bert-base-cased\"\n",
    "MAX_LENGTH = 128\n",
    "    \n",
    "inp = Path(INPUT_JSON_PATH)\n",
    "assert inp.exists(), f\"Input {inp} not found.\"\n",
    "data = load_json(inp)\n",
    "print(f\"Loaded {len(data)} examples from {inp}\")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "tokenized, problems = convert_spans_to_token_labels(data, tokenizer, MAX_LENGTH)\n",
    "print(f\"Converted {len(tokenized['input_ids'])} examples to token labels. Problematic entities: {len(problems)}\")\n",
    "if problems:\n",
    "    print(\"Sample problem:\", problems[:3])\n",
    "\n",
    "# write conll for inspection\n",
    "conll_path = write_conll(OUTPUT_PATH, tokenized[\"tokens\"], tokenized[\"labels\"], tokenizer)\n",
    "print(f\"Wrote inspection CoNLL to: {conll_path}\")\n",
    "\n",
    "# save HF dataset if possible\n",
    "outdir = save_hf_dataset(OUTPUT_PATH, tokenized)\n",
    "if outdir:\n",
    "    print(f\"Hugging Face dataset saved to: {outdir}\")\n",
    "\n",
    "# print a few token/label samples\n",
    "n_show = min(3, len(tokenized[\"tokens\"]))\n",
    "print(\"\\n=== Example token/label outputs (first %d) ===\" % n_show)\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "for i in range(n_show):\n",
    "    print(f\"\\nTEXT: {tokenized['text'][i]}\")\n",
    "    toks = tokenized[\"tokens\"][i]\n",
    "    labs = tokenized[\"labels\"][i]\n",
    "    pretty = []\n",
    "    for t, l in zip(toks, labs):\n",
    "        if l == -100:\n",
    "            continue\n",
    "        pretty.append(f\"{t}:{id2label.get(l,'-')}\")\n",
    "    print(\" \".join(pretty))\n",
    "\n",
    "# print trainer-friendly mapping\n",
    "print(\"\\nLabel mapping for Trainer:\")\n",
    "id2label_trainer = {str(v): k for k, v in label2id.items()}\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label_trainer)\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
